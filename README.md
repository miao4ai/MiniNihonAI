# MiniNihonAI

> Lightweight AI Infrastructure for Japanese-Language Applications, Built for Extensible Multimodal Agents

---

## Overview

**MiniNihonAI** is a modular, lightweight AI infrastructure framework tailored for Japanese-language applications. It is optimized for local deployment (e.g., RTX 4090) and cloud elasticity, and serves as a robust backend engine for building retrieval-augmented generation (RAG) systems, multimodal assistants, and future autonomous agent applications.

This repository focuses **solely on the Infra layer**â€”apps like tourism assistants or autonomous driving agents will be built *on top* of this infrastructure using its modular APIs.

---

## Core Features

- âœ… Local-first deployment with optional cloud extensions
- âœ… Japanese LLM support (Mistral 7B, Phi-2, Elyza-Japanese)
- âœ… Embedding & RAG engine with FAISS/Qdrant
- âœ… Optional multimodal processing (CLIP, BLIP2)
- âœ… Modular FastAPI endpoints (e.g., `/ask`, `/embed`, `/image_caption`)
- âœ… PEFT-friendly architecture (LoRA/QLoRA compatible)
- âœ… Extensible for downstream Agents: tourism bots, autonomous vehicle assistants, etc.

---

## Directory Structure (Full)

```
MiniNihonAI/
â”œâ”€â”€ api/                        # FastAPI server and route logic
â”‚   â””â”€â”€ main.py                # Entry point for API
â”‚   â””â”€â”€ routers/               # Route modules
â”‚       â””â”€â”€ qa_router.py       # QA endpoint logic
â”‚
â”œâ”€â”€ configs/                   # Configurations
â”‚   â””â”€â”€ config.yaml            # All model and service configs
â”‚
â”œâ”€â”€ data/                      # Data storage and examples
â”‚   â”œâ”€â”€ documents/             # Raw and cleaned knowledge docs
â”‚   â”œâ”€â”€ images/                # Sample tourist images
â”‚   â””â”€â”€ index/                 # FAISS / Qdrant vector indexes
â”‚
â”œâ”€â”€ deploy/                    # Deployment scripts
â”‚   â”œâ”€â”€ docker-compose.yml     # Docker infra
â”‚   â””â”€â”€ aws_launcher.sh        # AWS EC2 setup script
â”‚
â”œâ”€â”€ models/                    # Model loaders
â”‚   â”œâ”€â”€ llm_loader.py          # Load and run LLMs
â”‚   â”œâ”€â”€ embed_loader.py        # Load embedding models
â”‚   â”œâ”€â”€ clip_loader.py         # For multimodal (image-text)
â”‚   â””â”€â”€ translator.py          # Optional multilingual tools
â”‚
â”œâ”€â”€ rag/                       # Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ retriever.py           # Vector search logic
â”‚   â””â”€â”€ rag_pipeline.py        # QA pipeline with generator
â”‚
â”œâ”€â”€ agent/                     # (Experimental) Agent logic
â”‚   â”œâ”€â”€ intent_router.py       # Convert natural language into structured control commands
â”‚   â””â”€â”€ motion_planner.py      # Motion planning module for autonomous navigation
â”‚
â”œâ”€â”€ ui/                        # (Optional) Gradio/Streamlit UI
â”‚   â”œâ”€â”€ gradio_ui.py
â”‚   â””â”€â”€ streamlit_ui.py
â”‚
â”œâ”€â”€ utils/                     # Utilities
â”‚   â”œâ”€â”€ logger.py              # Logging utility
â”‚   â”œâ”€â”€ splitter.py            # Text splitter for RAG
â”‚   â””â”€â”€ file_loader.py         # File and doc loader
â”‚
â”œâ”€â”€ vector_store/             # Vector DB interfaces
â”‚   â”œâ”€â”€ faiss_engine.py        # FAISS wrapper
â”‚   â””â”€â”€ qdrant_engine.py       # Qdrant wrapper
â”‚
â”œâ”€â”€ logs/                     # Logging outputs
â”‚
â”œâ”€â”€ .env                      # Environment variables (API keys, etc)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

---

## Use Cases Enabled by This Infra

- ðŸ—ºï¸ **Tourism Agents** â€“ Japanese Q&A over local spots, routes, food
- ðŸ–¼ï¸ **Multimodal Search** â€“ Upload images and retrieve nearby location info
- ðŸ’¬ **RAG QA Bots** â€“ Retrieval-augmented answering over structured or unstructured Japanese knowledge
- ðŸˆ³ **On-Device LLM Inference** â€“ Low-latency GPT-style response generation
- ðŸš˜ **Autonomous Driving Agents (Prototype)** â€“ Natural language intent â†’ driving task interpretation pipeline
- ðŸ›£ï¸ **Motion Planning for Autonomous Agents** â€“ Waypoint-based path planning and behavioral routing (WIP)

---

## Agent-Oriented Extensions

This infra also supports building **autonomous or semi-autonomous agents**, such as:

### ðŸ§³ "Japanese Tourism Guide Agent"
> A cultural and location-aware assistant that handles tourist queries like â€œæŽ¨èäº¬éƒ½æ˜¥å¤©æ‹ç…§çš„æ™¯ç‚¹æœ‰å“ªäº›ï¼Ÿâ€ and:
- Answers using location-tagged documents, embedding search, and optional image input
- Recommends spots, routes, and real-time considerations like weather or crowding
- Can power mobile apps, web guides, or LINE bots

### ðŸ§­ "Japanese Autonomous Driving Agent"
> A prototype that interprets voice or text commands like â€œä»£ã€…æœ¨å…¬åœ’ã¾ã§é€£ã‚Œã¦ã£ã¦â€ and:
- Translates â†’ extracts intent â†’ queries embedded location docs
- Returns planned route or task
- Optionally connects to real/simulated vehicle SDKs

### ðŸ¤– "Japanese-English Translation Agent"
> A service agent capable of performing lightweight real-time or batch translation using a combination of LLMs, NLLB models, and a retrieval base.
- Translates and rewrites based on user intent and tone
- Can integrate with browser plugins or chat interfaces

### ðŸ± "Hotel Concierge Robot Agent"
> A multimodal assistant that receives text or image inputs to:
- Recommend nearby restaurants or services
- Answer common questions about bookings or hotel rules
- Generate suggested replies to Japanese-language guest queries

Modules used:
- `llm_loader.py`, `translator.py`, `vector_store/`
- `agent/intent_router.py`, `agent/motion_planner.py`
- Future extensions: `speech_interface.py`, `vehicle_api.py`, `task_manager.py`, `tts_engine.py`Future extensions: `speech_interface.py`, `vehicle_api.py`

---


---

## Deployment Targets

- âœ… Local machine (Linux with CUDA, Windows WSL2 supported)
- âœ… Docker Compose setup
- ðŸ”œ AWS EC2 GPU spot instance launcher

---

## License

MIT License (c) 2025 Miao Jiang.

