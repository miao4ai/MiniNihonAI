# MiniNihonAI

> Lightweight AI Infrastructure for Japanese-Language Applications, Built for Extensible Multimodal Agents

---

## Overview

**MiniNihonAI** is a modular, lightweight AI infrastructure framework tailored for Japanese-language applications. It is optimized for local deployment (e.g., RTX 4090) and cloud elasticity, and serves as a robust backend engine for building retrieval-augmented generation (RAG) systems, multimodal assistants, and future autonomous agent applications.

This repository focuses **solely on the Infra layer**â€”apps like tourism assistants or autonomous driving agents will be built *on top* of this infrastructure using its modular APIs.

---

## Core Features

- âœ… Local-first deployment with optional cloud extensions
- âœ… Japanese LLM support (Mistral 7B, Phi-2, Elyza-Japanese)
- âœ… Embedding & RAG engine with FAISS/Qdrant
- âœ… Optional multimodal processing (CLIP, BLIP2)
- âœ… Modular FastAPI endpoints (e.g., `/ask`, `/embed`, `/image_caption`)
- âœ… PEFT-friendly architecture (LoRA/QLoRA compatible)
- âœ… Extensible for downstream Agents: tourism bots, autonomous vehicle assistants, etc.

---

## Directory Structure (Full)

```
MiniNihonAI/
â”œâ”€â”€ api/                        # FastAPI server and route logic
â”‚   â””â”€â”€ main.py                # Entry point for API
â”‚   â””â”€â”€ routers/               # Route modules
â”‚       â””â”€â”€ qa_router.py       # QA endpoint logic
â”‚
â”œâ”€â”€ configs/                   # Configurations
â”‚   â””â”€â”€ config.yaml            # All model and service configs
â”‚
â”œâ”€â”€ data/                      # Data storage and examples
â”‚   â”œâ”€â”€ documents/             # Raw and cleaned knowledge docs
â”‚   â”œâ”€â”€ images/                # Sample tourist images
â”‚   â””â”€â”€ index/                 # FAISS / Qdrant vector indexes
â”‚
â”œâ”€â”€ deploy/                    # Deployment scripts
â”‚   â”œâ”€â”€ docker-compose.yml     # Docker infra
â”‚   â””â”€â”€ aws_launcher.sh        # AWS EC2 setup script
â”‚
â”œâ”€â”€ models/                    # Model loaders
â”‚   â”œâ”€â”€ llm_loader.py          # Load and run LLMs
â”‚   â”œâ”€â”€ embed_loader.py        # Load embedding models
â”‚   â”œâ”€â”€ clip_loader.py         # For multimodal (image-text)
â”‚   â””â”€â”€ translator.py          # Optional multilingual tools
â”‚
â”œâ”€â”€ rag/                       # Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ retriever.py           # Vector search logic
â”‚   â””â”€â”€ rag_pipeline.py        # QA pipeline with generator
â”‚
â”œâ”€â”€ agent/                     # (Experimental) Agent logic
â”‚   â””â”€â”€ intent_router.py       # Convert natural language into structured control commands
â”‚
â”œâ”€â”€ ui/                        # (Optional) Gradio/Streamlit UI
â”‚   â”œâ”€â”€ gradio_ui.py
â”‚   â””â”€â”€ streamlit_ui.py
â”‚
â”œâ”€â”€ utils/                     # Utilities
â”‚   â”œâ”€â”€ logger.py              # Logging utility
â”‚   â”œâ”€â”€ splitter.py            # Text splitter for RAG
â”‚   â””â”€â”€ file_loader.py         # File and doc loader
â”‚
â”œâ”€â”€ vector_store/             # Vector DB interfaces
â”‚   â”œâ”€â”€ faiss_engine.py        # FAISS wrapper
â”‚   â””â”€â”€ qdrant_engine.py       # Qdrant wrapper
â”‚
â”œâ”€â”€ logs/                     # Logging outputs
â”‚
â”œâ”€â”€ .env                      # Environment variables (API keys, etc)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

---

## Use Cases Enabled by This Infra

- ğŸ—ºï¸ **Tourism Agents** â€“ Japanese Q&A over local spots, routes, food
- ğŸ–¼ï¸ **Multimodal Search** â€“ Upload images and retrieve nearby location info
- ğŸ’¬ **RAG QA Bots** â€“ Retrieval-augmented answering over structured or unstructured Japanese knowledge
- ğŸˆ³ **On-Device LLM Inference** â€“ Low-latency GPT-style response generation
- ğŸš˜ **Autonomous Driving Agents (Prototype)** â€“ Natural language intent â†’ driving task interpretation pipeline

---

## Agent-Oriented Extensions

This infra also supports building **autonomous or semi-autonomous agents**, such as:

### ğŸ§­ "Japanese Autonomous Driving Agent"
> A prototype that interprets voice or text commands like â€œä»£ã€…æœ¨å…¬åœ’ã¾ã§é€£ã‚Œã¦ã£ã¦â€ and:
- Translates â†’ extracts intent â†’ queries embedded location docs
- Returns planned route or task
- Optionally connects to real/simulated vehicle SDKs

Modules used:
- `llm_loader.py`, `translator.py`, `vector_store/`
- `agent/intent_router.py`
- Future extensions: `planner.py`, `speech_interface.py`, `vehicle_api.py`

## Deployment Targets

- âœ… Local machine (Linux with CUDA, Windows WSL2 supported)
- âœ… Docker Compose setup
- ğŸ”œ AWS EC2 GPU spot instance launcher

---

## License

MIT License (c) 2025 Miao Jiang Kaggle Win Inc.

